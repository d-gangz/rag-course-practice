# RAG Implementation Practice

Hands-on learning for production-grade RAG systems with emphasis on evaluation, optimization, and statistical validation.

## ğŸ“š Overview

Progressive weekly modules covering:
- **Week 0**: Vector databases (LanceDB)
- **Week 1**: Synthetic test generation â†’ Retrieval benchmarking â†’ Statistical validation

**Core Focus:** Prove your RAG improvements are real, not just noise.

---

## ğŸš€ Quick Start

```bash
# Setup
git clone <repo-url> && cd rag-course-practice
uv venv && uv pip install -r requirements.txt
cp .env.example .env  # Add your OpenAI API key

# Run Week 1 workflow
uv run week-1/01_syn_ques_eg.py      # Generate test questions
uv run week-1/02_bench_retrieve.py   # Benchmark configurations
uv run week-1/03_visualise_results.py # Statistical analysis
```

---

## ğŸ“ Project Structure

```
week-0/
  â””â”€â”€ lancedb_example.py          # Vector DB basics

week-1/
  â”œâ”€â”€ 01_syn_ques_eg.py           # Generate evaluation datasets
  â”œâ”€â”€ 02_bench_retrieve.py        # Benchmark 8 configurations
  â”œâ”€â”€ 03_visualise_results.py     # Bootstrapping & significance tests
  â”œâ”€â”€ 02_learnings.md             # Retrieval metrics guide
  â”œâ”€â”€ 03_learnings.md             # Statistical validation guide â­
  â””â”€â”€ semaphore.md                # AsyncIO patterns
```

---

## ğŸ¯ What You'll Learn

### Week 1: Statistical Validation of RAG Systems

**The Problem:** You test two models and see:
- Model A: recall@10 = 0.847
- Model B: recall@10 = 0.851

Is Model B actually better, or just lucky?

**The Solution:** Bootstrapping + Statistical Testing

```python
# After 1,000 bootstrap simulations:
Model A: 0.847 Â± 0.027 (CI: [0.820, 0.874])
Model B: 0.851 Â± 0.026 (CI: [0.825, 0.877])
p-value: 0.234 (NOT significant)

Decision: âŒ Don't deploy Model B (improvement is noise)
```

**Files:**
- `03_visualise_results.py` - Implementation
- `03_learnings.md` - Complete guide (when to use, sample sizes, decision framework)

---

## ğŸ“Š Evaluation Workflow

```bash
# 1. Generate test dataset (100 questions)
uv run week-1/01_syn_ques_eg.py

# 2. Test configurations (8 experiments: 2 rerankers Ã— 2 search modes Ã— 2 embeddings)
uv run week-1/02_bench_retrieve.py

# 3. Statistical validation
uv run week-1/03_visualise_results.py
# â†’ Bootstrapping (1,000 simulations)
# â†’ Confidence intervals
# â†’ p-values
# â†’ Plots with uncertainty bands

# 4. Make decision
# If p < 0.05 AND worth the cost â†’ Deploy âœ…
# If p > 0.05 â†’ Don't deploy (noise) âŒ
```

---

## ğŸ“– Key Metrics

**Recall@k**: Did we find the correct answer in top k results?
```python
recall@10 = 0.85  # 85% of queries had correct answer in top 10
```

**MRR (Mean Reciprocal Rank)**: How high did we rank the correct answer?
```python
mrr@10 = 0.75  # Average position = 1.33 (1/0.75)
```

---

## ğŸ“ Key Takeaways

1. **Don't trust single numbers** - Use confidence intervals
   ```python
   âŒ "recall@10 = 0.85"
   âœ… "recall@10 = 0.85 Â± 0.03 (CI: [0.82, 0.88])"
   ```

2. **Statistical significance â‰  practical significance**
   - Can be significant (p < 0.05) but not worth deploying (tiny improvement, high cost)

3. **Sample size matters**
   - < 30 questions: Too small
   - 50-100: Reasonable
   - 100+: Good
   - 10,000+: Skip bootstrapping

4. **Context over p-values**
   - Free upgrade with p=0.08? â†’ Deploy anyway
   - 6.5x cost with p=0.23? â†’ Don't deploy

---

## ğŸ› ï¸ Development

```bash
# Code quality
uv run black <file.py>
uv run ruff <file.py>
uv run mypy <file.py>

# Environment variables (.env)
OPENAI_API_KEY=sk-...
BRAINTRUST_API_KEY=...  # Optional
COHERE_API_KEY=...      # Optional
```

---

## ğŸ“š Essential Reading

- **[03_learnings.md](week-1/03_learnings.md)** - Complete statistical validation guide
  - When to use bootstrapping
  - How to interpret p-values
  - Sample size guidelines
  - 5 real-world decision examples

- **[semaphore.md](week-1/semaphore.md)** - AsyncIO concurrency patterns

---

## ğŸ”§ Troubleshooting

**Rate limits**: Reduce `Semaphore(10)` â†’ `Semaphore(5)`
**Memory**: Process in smaller batches
**Dependencies**: `rm -rf .venv && uv venv && uv pip install -r requirements.txt`

---

## ğŸ¤ Contributing

1. Create file in `week-X/`
2. Add docstring header (see existing files)
3. Update README
4. Create `XX_learnings.md` with insights

---

**"Don't trust a single number. Get the distribution. Test significance. Make data-driven decisions."**
